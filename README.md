# RAG-based Interface Agent ğŸ¤–

**SystÃ¨me Intelligent de PrÃ©diction de Domaines IT** basÃ© sur une architecture RAG (Retrieval-Augmented Generation)

Un outil interactif qui aide les utilisateurs Ã  identifier les domaines IT qui correspondent le mieux Ã  leur profil et leurs prÃ©fÃ©rences par le biais d'un questionnaire intelligent.

---

## ğŸ“‹ Vue d'ensemble

Ce projet combine :
- **Questionnaire adaptatif** : Pose des questions ciblÃ©es sur les prÃ©fÃ©rences IT
- **Vector Store (ChromaDB)** : Stocke les domaines IT et recherche les correspondances
- **LLM Ollama** : Analyse les rÃ©ponses avec un modÃ¨le de langage local
- **RAG** : RÃ©cupÃ¨re les domaines pertinents et gÃ©nÃ¨re des recommandations personnalisÃ©es

### Domaines IT couverts
- â˜ï¸ Cloud et Automatisation
- ğŸ”’ CybersÃ©curitÃ©
- ğŸ“Š Data Science et IA
- ğŸš€ DevOps
- ğŸŒ RÃ©seaux informatiques
- ğŸ’» DÃ©veloppement logiciel

---

## ğŸš€ Installation

### 1. PrÃ©requis
- **Python 3.8+**
- **Ollama** (pour le LLM local) â†’ [TÃ©lÃ©charger ici](https://ollama.ai)
- **pip** (gestionnaire de paquets Python)

### 2. Cloner le projet

```bash
git clone https://github.com/Adaptative-Learning/RAG-based-Interface-Agent.git
cd RAG-based-Interface-Agent
git checkout ollama  # Switch to ollama branch
```

### 3. Installer les dÃ©pendances

```bash
pip install -r requirements.txt
```

**DÃ©pendances principales :**
- `chromadb` : Base de donnÃ©es vectorielle pour la recherche sÃ©mantique
- `requests` : Pour communiquer avec Ollama

### 4. Configurer Ollama

**Installation d'Ollama :**
1. TÃ©lÃ©chargez Ollama depuis [ollama.ai](https://ollama.ai)
2. Installez-le sur votre machine
3. Lancez le service Ollama
4. TÃ©lÃ©chargez un modÃ¨le lÃ©ger :

```bash
ollama pull llama3.2:1b
```

> **Note** : Le modÃ¨le `llama3.2:1b` est trÃ¨s lÃ©ger et rapide. Vous pouvez essayer d'autres modÃ¨les comme `mistral`, `neural-chat`, etc.

---

## ğŸ“– Utilisation

### Lancer l'application

```bash
python src/main.py
```

### Flux d'exÃ©cution

1. **Chargement de la base de connaissances** ğŸ”„
   - Les domaines IT sont chargÃ©s depuis `data/domaines/`
   - Une base vectorielle est crÃ©Ã©e pour la recherche sÃ©mantique

2. **Questionnaire interactif** â“
   - RÃ©pondez aux 10+ questions sur vos prÃ©fÃ©rences IT
   - Pour chaque question, choisissez l'option (1, 2, 3, etc.) qui vous correspond

3. **Analyse avec LLM** ğŸ§ 
   - Ollama analyse vos rÃ©ponses
   - Recherche les domaines pertinents dans la base vectorielle
   - GÃ©nÃ¨re une recommandation personnalisÃ©e

4. **RÃ©sultats** ğŸ“Š
   - Affichage du domaine recommandÃ©
   - Explication dÃ©taillÃ©e basÃ©e sur vos rÃ©ponses

### Exemple d'interaction

```
======================================================================
SYSTEME DE PREDICTION DE DOMAINES IT
======================================================================

[INFO] Chargement de la base de connaissances...
[OK] 6 domaines charges avec succes

[QUESTIONNAIRE] DEBUT DU QUESTIONNAIRE

Question 1: Comment prÃ©fÃ©rez-vous rÃ©soudre un problÃ¨me complexe dans un projet ?
1. Analyser les donnÃ©es disponibles pour trouver la solution
2. Coder une solution testable et itÃ©rative
3. Tester diffÃ©rentes hypothÃ¨ses rapidement pour identifier le problÃ¨me
4. Collaborer avec l'Ã©quipe pour brainstormer
5. Appliquer des mÃ©thodologies standardisÃ©es (agile, design thinking)

Choisis une option (numÃ©ro) : 2

...
```

---

## ğŸ“ Structure du projet

```
RAG-based-Interface-Agent/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                 # Point d'entrÃ©e principal
â”‚   â”œâ”€â”€ questionnaire.py        # Gestion du questionnaire
â”‚   â”œâ”€â”€ vector_store.py         # IntÃ©gration ChromaDB
â”‚   â””â”€â”€ llm_analyzer.py         # IntÃ©gration Ollama LLM
â”œâ”€â”€ data/
â”‚   â””â”€â”€ domaines/               # Fichiers texte des domaines IT
â”‚       â”œâ”€â”€ cloud.txt
â”‚       â”œâ”€â”€ cybersecurite.txt
â”‚       â”œâ”€â”€ data science.txt
â”‚       â”œâ”€â”€ devops.txt
â”‚       â”œâ”€â”€ reseaux.txt
â”‚       â””â”€â”€ software.txt
â”œâ”€â”€ questionnaire.json          # DÃ©finition des questions
â”œâ”€â”€ requirements.txt            # DÃ©pendances Python
â””â”€â”€ README.md                   # Ce fichier
```

---

## ğŸ”‘ Fichiers clÃ©s

### `questionnaire.json`
DÃ©finit les 10+ questions posÃ©es aux utilisateurs avec les domaines liÃ©s :
```json
{
  "questions": [
    {
      "id": 1,
      "question": "Comment prÃ©fÃ©rez-vous rÃ©soudre un problÃ¨me complexe ?",
      "options": ["Option 1", "Option 2", ...],
      "linked_domains": ["DÃ©veloppement logiciel", "Data et IA"]
    }
  ]
}
```

### `data/domaines/`
Contient les descriptions textuelles de chaque domaine IT :
- **cloud.txt** : Cloud, containerisation, infrastructure
- **cybersecurite.txt** : SÃ©curitÃ©, chiffrement, audit
- **data science.txt** : ML, IA, analytics
- **devops.txt** : CI/CD, monitoring, automatisation
- **reseaux.txt** : RÃ©seaux, TCP/IP, routing
- **software.txt** : Programmation, frameworks, design

### `src/main.py`
Orchestre tout le flux :
1. Charge les domaines dans ChromaDB
2. Lance le questionnaire
3. Analyse avec Ollama
4. Affiche les recommandations

---

## ğŸ› ï¸ Personnalisation

### Ajouter une nouvelle question

Modifiez `questionnaire.json` :
```json
{
  "id": 11,
  "question": "Votre nouvelle question ?",
  "type": "multiple_choice",
  "options": [
    "Option 1",
    "Option 2",
    "Option 3"
  ],
  "linked_domains": ["Domaine 1", "Domaine 2"]
}
```

### Ajouter un nouveau domaine

1. CrÃ©ez un fichier texte dans `data/domaines/` (ex: `machine_learning.txt`)
2. DÃ©crivez le domaine avec des dÃ©tails pertinents
3. Le domaine sera automatiquement chargÃ© au prochain lancement

### Changer le modÃ¨le LLM

Dans `src/main.py`, modifiez :
```python
analyzer = LLMAnalyzer(model="mistral")  # ou un autre modÃ¨le Ollama
```

ModÃ¨les recommandÃ©s :
- `llama3.2:1b` (trÃ¨s rapide, lÃ©ger)
- `mistral` (Ã©quilibrÃ©)
- `llama3.2:7b` (plus puissant, demande plus de RAM)

---

## âš™ï¸ DÃ©pannage

### Erreur : "Connection refused" (Ollama)
**Solution :**
```bash
# VÃ©rifiez qu'Ollama est en cours d'exÃ©cution
ollama serve
# Dans un autre terminal
ollama pull llama3.2:1b
```

### Erreur : "Module not found"
**Solution :**
```bash
pip install -r requirements.txt
```

### Performances lentes
- Utilisez un modÃ¨le plus lÃ©ger : `ollama pull llama3.2:1b`
- Augmentez la RAM disponible
- VÃ©rifiez votre CPU

### ChromaDB ne charge pas les domaines
**Solution :**
```bash
# VÃ©rifiez que les fichiers existent
ls data/domaines/
# Assurez-vous que les fichiers ne sont pas vides
```

---

## ğŸ”„ Architecture

```
User Input (Questionnaire)
        â†“
    Responses
        â†“
Vector Store (ChromaDB) â† Similarity Search
        â†“
LLM Analyzer (Ollama) â† Contextual Analysis
        â†“
Recommendations Output
```

### Flux RAG :
1. **Retrieval** : Recherche les domaines similaires aux rÃ©ponses
2. **Augmentation** : Enrichit le contexte avec le contenu des domaines
3. **Generation** : GÃ©nÃ¨re une rÃ©ponse personnalisÃ©e avec Ollama

---

## ğŸ“Š Branche `ollama`

Vous Ãªtes sur la branche `ollama` qui contient :
- âœ… IntÃ©gration complÃ¨te d'Ollama
- âœ… ModÃ¨les lÃ©gers optimisÃ©s
- âœ… Architecture RAG fonctionnelle
- âœ… Questionnaire adaptatif

---

## ğŸ¤ Contribution

Pour contribuer :
1. CrÃ©ez une branche (`git checkout -b feature/ma-feature`)
2. Committez vos changements (`git commit -m "Add: ma feature"`)
3. Poussez la branche (`git push origin feature/ma-feature`)
4. CrÃ©ez une Pull Request

---

## ğŸ“ Licence

Ce projet est fourni Ã  titre Ã©ducatif.

---

## ğŸ“ Support

Pour toute question ou problÃ¨me :
- VÃ©rifiez le section **DÃ©pannage**
- Consultez les logs d'exÃ©cution
- VÃ©rifiez que toutes les dÃ©pendances sont installÃ©es

---

## ğŸ¯ AmÃ©liorations futures

- [ ] Interface web avec Streamlit
- [ ] Stockage des rÃ©sultats en base de donnÃ©es
- [ ] ModÃ¨les multi-langues
- [ ] Historique des questionnaires
- [ ] Analyse comparative des domaines

---

**DerniÃ¨re mise Ã  jour** : DÃ©cembre 2025  
**Branche active** : `ollama`
